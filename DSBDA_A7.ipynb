{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKDONhc8wo+qOkc77vzF6f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment 7\n","Title-\n","1. Extract Sample document and apply following document preprocessing methods:\n","Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n","2. Create representation of document by calculating Term Frequency and Inverse Document\n","Frequency."],"metadata":{"id":"QtXwXiRHF2Vo"}},{"cell_type":"code","source":["!pip install nltk\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"id":"28In5nKZLbUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Mc6EtrnFDZux","executionInfo":{"status":"ok","timestamp":1713795549974,"user_tz":-330,"elapsed":7,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"outputs":[],"source":["# Import necessary libraries\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk import pos_tag\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","source":["# Sample document\n","sample_document = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.\"\n"],"metadata":{"id":"g98sCTEFGYhU","executionInfo":{"status":"ok","timestamp":1713795554415,"user_tz":-330,"elapsed":4,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Tokenization\n","tokens = word_tokenize(sample_document.lower())"],"metadata":{"id":"g7tqz8anGfp7","executionInfo":{"status":"ok","timestamp":1713795555109,"user_tz":-330,"elapsed":5,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# POS Tagging\n","pos_tags = pos_tag(tokens)"],"metadata":{"id":"1bbB4srjGiFl","executionInfo":{"status":"ok","timestamp":1713795555110,"user_tz":-330,"elapsed":6,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Stop Words Removal\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [token for token in tokens if token not in stop_words]"],"metadata":{"id":"fgDBGV6DGmGS","executionInfo":{"status":"ok","timestamp":1713795555757,"user_tz":-330,"elapsed":5,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Stemming\n","stemmer = PorterStemmer()\n","stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]"],"metadata":{"id":"xPayMtMgGoDM","executionInfo":{"status":"ok","timestamp":1713795555757,"user_tz":-330,"elapsed":5,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]"],"metadata":{"id":"vrnV0DIXGpsd","executionInfo":{"status":"ok","timestamp":1713795555757,"user_tz":-330,"elapsed":4,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# TF-IDF Representation\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(lemmatized_tokens)])"],"metadata":{"id":"mTwxP3_OGrxK","executionInfo":{"status":"ok","timestamp":1713795556765,"user_tz":-330,"elapsed":2,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Print Preprocessed Document\n","print(\"Preprocessed Document:\")\n","print(\"Tokenization:\", tokens)\n","print(\"POS Tagging:\", pos_tags)\n","print(\"Stop Words Removal:\", filtered_tokens)\n","print(\"Stemming:\", stemmed_tokens)\n","print(\"Lemmatization:\", lemmatized_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"D4Glh0WXGuyz","executionInfo":{"status":"ok","timestamp":1713795557715,"user_tz":-330,"elapsed":952,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}},"outputId":"1a57b4c3-0707-44e0-f403-5ad4d3d71045"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed Document:\n","Tokenization: ['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', '.']\n","POS Tagging: [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('nlp', 'JJ'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), (',', ','), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n","Stop Words Removal: ['natural', 'language', 'processing', '(', 'nlp', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', '.']\n","Stemming: ['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'linguist', ',', 'comput', 'scienc', ',', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', '.']\n","Lemmatization: ['natural', 'language', 'processing', '(', 'nlp', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', '.']\n"]}]},{"cell_type":"code","source":["# Print TF-IDF Representation\n","print(\"\\nTF-IDF Representation:\")\n","print(tfidf_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"AbR7avNDGwi1","executionInfo":{"status":"ok","timestamp":1713795557716,"user_tz":-330,"elapsed":4,"user":{"displayName":"Umama Patel","userId":"14801377007133337759"}},"outputId":"2681aa2a-06da-4677-8653-ffb54c83f5e3"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","TF-IDF Representation:\n","[[0.22941573 0.45883147 0.22941573 0.22941573 0.22941573 0.22941573\n","  0.45883147 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n","  0.22941573]]\n"]}]},{"cell_type":"markdown","source":["Observation -\n","1. Text preprocessing is crucial in natural language processing tasks to clean and normalize text data for further analysis.\n","2. The TF-IDF representation helps in measure the importance of each term in the document relative to the entire corpus, which can be useful for tasks like text classification and information retrieval.\n","3. The preprocessed document can be further used for various NLP tasks, such as sentiment analysis, text summarization, and document clustering."],"metadata":{"id":"uVNYYsclIP9M"}}]}